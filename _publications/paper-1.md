---
title: "Unsupervised Speaker Cue Usage Detection in Public Speaking Videos"
collection: publications
date: 2018-09-06
venue: 'BMVC'
citation: '<b>Gupta, A.</b> & Jayagopi, D. (2018). &quot;Unsupervised Speaker Cue Usage Detection in Public Speaking Videos.&quot; <i>Proceedings of the 29th British Machine Vision Conference Workshop, Vision for Interaction and Behaviour undErstanding</i>.'
---
**Abstract**: A speaker’s bodily cues such as walking, arm movement and head movement play a
big role in establishing engagement with the audience. Low level, pose keypoint features
to capture these cues have been used in prior studies to characterise engagement and
ratings, but are typically not interpretable, and have not been subjected to analysis to
understand their meaning. We thus apply a completely unsupervised approach on these
low level features to obtain easily computable higher level features that represent low,
medium, and high cue usage by the speaker. We apply our approach to classroom
recorded lectures and the significantly more difficult dataset of TED videos, and are able
to positively correlate our features to human interpretable ideas of a speaker’s lateral head
motion, and movement. We hope that the interpretable nature of these features can be
used in future work to serve as a means of feedback to speakers, and to better understand
the underlying structure behind the results.

[Download paper here](http://anshul-gupta24.github.io/files/paper1.pdf)
